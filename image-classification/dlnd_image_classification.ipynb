{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:16, 10.2MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a0ed7fd68>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "encoder = None\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    global encoder\n",
    "    if encoder is None:\n",
    "        encoder = preprocessing.LabelBinarizer().fit(x)\n",
    "    return encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32,shape=[None]+list(image_shape),name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32,shape=[None]+[n_classes],name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(dtype=tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_depth = int(x_tensor.get_shape()[3])\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize,input_depth,conv_num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    # Apply Convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1,*conv_strides,1], padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    # Apply Max Pooling\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1,*pool_ksize,1],\n",
    "    strides=[1,*pool_strides,1],\n",
    "    padding='SAME')\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    # fully connected layer\n",
    "    fully_conn_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return fully_conn_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    # fully connected layer\n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 128\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (2,2)\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (1,1)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    conv2d_maxpool_layer = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened_layer = flatten(conv2d_maxpool_layer)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_conn_layer = fully_conn(flattened_layer, num_outputs)\n",
    "    fully_conn_layer = tf.nn.dropout(fully_conn_layer, keep_prob)\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    output_layer = output(fully_conn_layer, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return output_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x:feature_batch,y:label_batch,keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict={x:valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    print('Loss: ',loss, 'Accuracy: ',validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 300\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  282.459 Accuracy:  0.1444\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  212.774 Accuracy:  0.182\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  184.308 Accuracy:  0.2084\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  148.722 Accuracy:  0.2286\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  121.179 Accuracy:  0.2422\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  116.594 Accuracy:  0.259\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  92.214 Accuracy:  0.2614\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  87.0262 Accuracy:  0.2522\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  73.1058 Accuracy:  0.2768\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  67.1735 Accuracy:  0.2776\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  62.4125 Accuracy:  0.3032\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  54.4072 Accuracy:  0.294\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  52.9467 Accuracy:  0.3002\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  48.0513 Accuracy:  0.3206\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  41.752 Accuracy:  0.3144\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  42.9039 Accuracy:  0.3278\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  37.021 Accuracy:  0.3202\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  35.4051 Accuracy:  0.3456\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  32.5941 Accuracy:  0.3298\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  31.0292 Accuracy:  0.3416\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  27.5944 Accuracy:  0.3484\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  27.9338 Accuracy:  0.3526\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  23.2945 Accuracy:  0.347\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  23.5979 Accuracy:  0.3618\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  19.6747 Accuracy:  0.356\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  19.677 Accuracy:  0.3586\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  16.9502 Accuracy:  0.3562\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  17.5111 Accuracy:  0.353\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  16.4827 Accuracy:  0.3476\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  13.0578 Accuracy:  0.3498\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  12.0021 Accuracy:  0.372\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  11.3451 Accuracy:  0.3562\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  9.48941 Accuracy:  0.346\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  8.90894 Accuracy:  0.3622\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  7.9624 Accuracy:  0.3628\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  7.2769 Accuracy:  0.3542\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  6.29904 Accuracy:  0.3528\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  5.85163 Accuracy:  0.3488\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  4.43933 Accuracy:  0.3648\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  4.22531 Accuracy:  0.3662\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  4.22822 Accuracy:  0.3624\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  3.41522 Accuracy:  0.3664\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  3.53377 Accuracy:  0.3654\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  3.74797 Accuracy:  0.3622\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  3.10078 Accuracy:  0.3766\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  3.21631 Accuracy:  0.3666\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  3.17363 Accuracy:  0.3774\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  3.14215 Accuracy:  0.376\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  2.54761 Accuracy:  0.3766\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  3.67738 Accuracy:  0.3842\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  2.75106 Accuracy:  0.3724\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  2.65177 Accuracy:  0.387\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  2.29481 Accuracy:  0.382\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  2.18087 Accuracy:  0.3748\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  2.15516 Accuracy:  0.3824\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  2.41351 Accuracy:  0.3826\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  2.27019 Accuracy:  0.3798\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  1.95307 Accuracy:  0.3928\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  2.03956 Accuracy:  0.3806\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  2.16554 Accuracy:  0.3882\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  2.34097 Accuracy:  0.378\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  1.95027 Accuracy:  0.392\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  1.84604 Accuracy:  0.3824\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  1.86979 Accuracy:  0.3886\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  1.50949 Accuracy:  0.4064\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  1.97939 Accuracy:  0.3936\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  1.65405 Accuracy:  0.4\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  2.94668 Accuracy:  0.3898\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  1.4365 Accuracy:  0.3906\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  1.85105 Accuracy:  0.3998\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  1.80719 Accuracy:  0.3924\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  1.62981 Accuracy:  0.3936\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  1.44538 Accuracy:  0.408\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  1.55923 Accuracy:  0.3852\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  1.5957 Accuracy:  0.396\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  1.81851 Accuracy:  0.3968\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  2.11781 Accuracy:  0.4094\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  2.05341 Accuracy:  0.409\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  1.57302 Accuracy:  0.4024\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  1.44854 Accuracy:  0.4096\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  1.42281 Accuracy:  0.4012\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  1.63013 Accuracy:  0.403\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  1.44205 Accuracy:  0.42\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  1.41644 Accuracy:  0.4126\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  1.57302 Accuracy:  0.4212\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  1.4258 Accuracy:  0.4064\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  1.67223 Accuracy:  0.4068\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  1.40972 Accuracy:  0.4064\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  1.44056 Accuracy:  0.4126\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  1.83322 Accuracy:  0.4128\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  2.15507 Accuracy:  0.4104\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  1.41434 Accuracy:  0.4328\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  1.37437 Accuracy:  0.4064\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  1.41226 Accuracy:  0.4184\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  1.44321 Accuracy:  0.4172\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  1.39976 Accuracy:  0.411\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  1.38325 Accuracy:  0.4322\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  1.37869 Accuracy:  0.427\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  1.33587 Accuracy:  0.424\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  1.33057 Accuracy:  0.431\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  1.45794 Accuracy:  0.415\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  1.3759 Accuracy:  0.4184\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  1.31992 Accuracy:  0.4408\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  1.37008 Accuracy:  0.426\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  1.34103 Accuracy:  0.433\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  1.33679 Accuracy:  0.437\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  1.30027 Accuracy:  0.443\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  1.32654 Accuracy:  0.4354\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  1.33388 Accuracy:  0.437\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  1.27163 Accuracy:  0.4334\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  1.28146 Accuracy:  0.44\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  1.31982 Accuracy:  0.4402\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  1.42955 Accuracy:  0.4438\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  1.29428 Accuracy:  0.4436\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  1.34269 Accuracy:  0.424\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  1.20878 Accuracy:  0.4482\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  1.32825 Accuracy:  0.4482\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  1.25915 Accuracy:  0.454\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  1.34096 Accuracy:  0.4416\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  1.2668 Accuracy:  0.448\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  1.19604 Accuracy:  0.4594\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  1.28222 Accuracy:  0.4472\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  1.24877 Accuracy:  0.4412\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  1.21126 Accuracy:  0.4478\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  1.22476 Accuracy:  0.4616\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  1.21798 Accuracy:  0.4546\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  1.193 Accuracy:  0.4638\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  1.16499 Accuracy:  0.4576\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  1.17814 Accuracy:  0.4532\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  1.21005 Accuracy:  0.459\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  1.20938 Accuracy:  0.4666\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  1.21907 Accuracy:  0.4466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  1.18182 Accuracy:  0.4654\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  1.15549 Accuracy:  0.4642\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  1.19762 Accuracy:  0.466\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  1.22506 Accuracy:  0.4658\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  1.24141 Accuracy:  0.465\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  1.1831 Accuracy:  0.464\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  1.24674 Accuracy:  0.4676\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  1.16922 Accuracy:  0.4746\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  1.16703 Accuracy:  0.4782\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  1.16183 Accuracy:  0.4834\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  1.15341 Accuracy:  0.4888\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  1.18193 Accuracy:  0.4838\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  1.17506 Accuracy:  0.4838\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  1.12474 Accuracy:  0.4788\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  1.12069 Accuracy:  0.492\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  1.07639 Accuracy:  0.4954\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  1.13474 Accuracy:  0.4946\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  1.10115 Accuracy:  0.4904\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  1.14179 Accuracy:  0.4934\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  1.10564 Accuracy:  0.4938\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  1.09344 Accuracy:  0.503\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  1.0457 Accuracy:  0.5146\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  1.10379 Accuracy:  0.5026\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  1.04218 Accuracy:  0.4942\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  1.01738 Accuracy:  0.5032\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  1.0524 Accuracy:  0.5026\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  1.06474 Accuracy:  0.5064\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  1.02986 Accuracy:  0.5006\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  1.03969 Accuracy:  0.491\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  1.02671 Accuracy:  0.5142\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  1.02932 Accuracy:  0.5038\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.949031 Accuracy:  0.5076\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.968853 Accuracy:  0.5258\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  1.00248 Accuracy:  0.5044\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.975893 Accuracy:  0.5072\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.975155 Accuracy:  0.5128\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.951786 Accuracy:  0.515\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.943866 Accuracy:  0.5162\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.967997 Accuracy:  0.5146\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.886458 Accuracy:  0.5088\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.966647 Accuracy:  0.513\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.997633 Accuracy:  0.51\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.892492 Accuracy:  0.5148\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.853483 Accuracy:  0.5126\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:  0.904936 Accuracy:  0.5172\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.855499 Accuracy:  0.522\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.872862 Accuracy:  0.5202\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.883522 Accuracy:  0.5308\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.859482 Accuracy:  0.516\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.845939 Accuracy:  0.531\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.844286 Accuracy:  0.528\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.858254 Accuracy:  0.5208\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.825588 Accuracy:  0.5144\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.862799 Accuracy:  0.5218\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.844149 Accuracy:  0.534\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.803786 Accuracy:  0.5226\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.854043 Accuracy:  0.5144\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.785935 Accuracy:  0.5284\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.791524 Accuracy:  0.5308\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.77021 Accuracy:  0.5238\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  0.799776 Accuracy:  0.5328\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  0.737574 Accuracy:  0.5192\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.739159 Accuracy:  0.5228\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.803852 Accuracy:  0.5272\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.687439 Accuracy:  0.5152\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.719859 Accuracy:  0.5188\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.77269 Accuracy:  0.511\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.741468 Accuracy:  0.5098\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.788858 Accuracy:  0.5248\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.744337 Accuracy:  0.528\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.662256 Accuracy:  0.5152\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.736353 Accuracy:  0.5308\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.731775 Accuracy:  0.5214\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.665534 Accuracy:  0.5386\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.653452 Accuracy:  0.5326\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.660626 Accuracy:  0.5218\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.688562 Accuracy:  0.5298\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.697821 Accuracy:  0.5266\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.688503 Accuracy:  0.5288\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.701514 Accuracy:  0.5322\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.650824 Accuracy:  0.5214\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.670412 Accuracy:  0.542\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.67685 Accuracy:  0.5226\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.685129 Accuracy:  0.5186\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.682517 Accuracy:  0.535\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.673094 Accuracy:  0.5402\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.67457 Accuracy:  0.53\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.61702 Accuracy:  0.528\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.626094 Accuracy:  0.5362\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.643137 Accuracy:  0.541\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.643281 Accuracy:  0.5364\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.58324 Accuracy:  0.5284\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.60579 Accuracy:  0.5242\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.64637 Accuracy:  0.539\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.603842 Accuracy:  0.5252\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.623282 Accuracy:  0.5402\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.571905 Accuracy:  0.535\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.561664 Accuracy:  0.525\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.588989 Accuracy:  0.5402\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.555087 Accuracy:  0.5374\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.550683 Accuracy:  0.527\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.579243 Accuracy:  0.5222\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.570708 Accuracy:  0.522\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.569138 Accuracy:  0.5194\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.566435 Accuracy:  0.5252\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.540784 Accuracy:  0.5348\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.55105 Accuracy:  0.5376\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.510356 Accuracy:  0.53\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.564312 Accuracy:  0.5248\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.571263 Accuracy:  0.528\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.53903 Accuracy:  0.5328\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.513956 Accuracy:  0.5104\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.515623 Accuracy:  0.5242\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.527059 Accuracy:  0.543\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.517373 Accuracy:  0.5448\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.500178 Accuracy:  0.542\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.476435 Accuracy:  0.5434\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.503683 Accuracy:  0.5354\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.520134 Accuracy:  0.5366\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.497053 Accuracy:  0.538\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.51326 Accuracy:  0.5436\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.442942 Accuracy:  0.543\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.51526 Accuracy:  0.5294\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.487872 Accuracy:  0.54\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:  0.461473 Accuracy:  0.5312\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:  0.483993 Accuracy:  0.5174\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:  0.478767 Accuracy:  0.522\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:  0.431313 Accuracy:  0.5194\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:  0.468803 Accuracy:  0.5306\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:  0.423289 Accuracy:  0.5284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263, CIFAR-10 Batch 1:  Loss:  0.45954 Accuracy:  0.5446\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:  0.434342 Accuracy:  0.5378\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:  0.455279 Accuracy:  0.5336\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:  0.435649 Accuracy:  0.5434\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:  0.428076 Accuracy:  0.5382\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:  0.435455 Accuracy:  0.5332\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:  0.419451 Accuracy:  0.5326\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:  0.407256 Accuracy:  0.533\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:  0.41034 Accuracy:  0.5182\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:  0.429718 Accuracy:  0.5332\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:  0.42342 Accuracy:  0.5358\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:  0.417269 Accuracy:  0.536\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:  0.39901 Accuracy:  0.534\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:  0.408296 Accuracy:  0.5344\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:  0.391245 Accuracy:  0.5438\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:  0.422907 Accuracy:  0.5188\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:  0.421529 Accuracy:  0.53\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:  0.399357 Accuracy:  0.534\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:  0.411721 Accuracy:  0.5486\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:  0.39167 Accuracy:  0.5392\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:  0.382956 Accuracy:  0.5366\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:  0.37058 Accuracy:  0.5368\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:  0.390188 Accuracy:  0.5324\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:  0.369178 Accuracy:  0.5276\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:  0.375406 Accuracy:  0.5258\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:  0.367548 Accuracy:  0.5418\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:  0.352084 Accuracy:  0.5312\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:  0.364566 Accuracy:  0.55\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:  0.344615 Accuracy:  0.5282\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:  0.368126 Accuracy:  0.5296\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:  0.350862 Accuracy:  0.522\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:  0.365557 Accuracy:  0.5418\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:  0.362904 Accuracy:  0.5472\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:  0.344079 Accuracy:  0.5274\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:  0.332636 Accuracy:  0.5284\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:  0.352811 Accuracy:  0.5348\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:  0.330913 Accuracy:  0.5362\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:  0.329051 Accuracy:  0.5452\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  231.632 Accuracy:  0.1522\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:  179.066 Accuracy:  0.1838\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:  143.778 Accuracy:  0.2096\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:  96.8516 Accuracy:  0.2234\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:  111.07 Accuracy:  0.2386\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  100.811 Accuracy:  0.227\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:  86.9052 Accuracy:  0.256\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:  76.1488 Accuracy:  0.2678\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:  57.2585 Accuracy:  0.2648\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:  75.8445 Accuracy:  0.2714\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  64.8666 Accuracy:  0.27\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:  51.3847 Accuracy:  0.276\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:  47.1565 Accuracy:  0.2966\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:  42.6438 Accuracy:  0.311\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:  49.7247 Accuracy:  0.3036\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  43.5724 Accuracy:  0.3002\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:  32.4521 Accuracy:  0.3024\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:  28.2928 Accuracy:  0.3104\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:  24.4513 Accuracy:  0.3298\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:  35.0287 Accuracy:  0.3124\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  29.9506 Accuracy:  0.3292\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:  23.7661 Accuracy:  0.3208\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:  17.9164 Accuracy:  0.3316\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:  16.8309 Accuracy:  0.3366\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:  22.7403 Accuracy:  0.3338\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  21.0636 Accuracy:  0.334\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:  14.3636 Accuracy:  0.3118\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:  11.3152 Accuracy:  0.3276\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:  9.96532 Accuracy:  0.3306\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:  13.7384 Accuracy:  0.311\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  12.8852 Accuracy:  0.3332\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:  8.10256 Accuracy:  0.308\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:  7.07134 Accuracy:  0.3264\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:  7.27262 Accuracy:  0.3136\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:  8.54471 Accuracy:  0.3056\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  7.89634 Accuracy:  0.3136\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:  4.95968 Accuracy:  0.3126\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:  4.89398 Accuracy:  0.3052\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:  4.25466 Accuracy:  0.3204\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:  5.46952 Accuracy:  0.3134\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  4.80095 Accuracy:  0.3286\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:  2.85594 Accuracy:  0.3076\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:  3.27619 Accuracy:  0.3164\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:  2.76571 Accuracy:  0.324\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:  3.10505 Accuracy:  0.3274\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  2.75792 Accuracy:  0.3322\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:  1.84905 Accuracy:  0.3242\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:  1.60988 Accuracy:  0.3226\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:  1.69335 Accuracy:  0.3374\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:  1.832 Accuracy:  0.334\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  1.89459 Accuracy:  0.3206\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:  1.7172 Accuracy:  0.339\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:  1.58563 Accuracy:  0.3396\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:  1.70788 Accuracy:  0.3516\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:  1.80239 Accuracy:  0.348\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  1.88513 Accuracy:  0.3528\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:  1.73399 Accuracy:  0.3578\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:  1.55436 Accuracy:  0.3446\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:  1.63093 Accuracy:  0.3494\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:  1.78222 Accuracy:  0.3588\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  1.86365 Accuracy:  0.3666\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:  1.72927 Accuracy:  0.3488\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:  1.51218 Accuracy:  0.3708\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:  1.56405 Accuracy:  0.3802\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:  1.75946 Accuracy:  0.3628\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  1.85167 Accuracy:  0.3806\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:  1.73 Accuracy:  0.3724\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:  1.48903 Accuracy:  0.3706\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:  1.54196 Accuracy:  0.3814\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:  1.72135 Accuracy:  0.372\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  1.85668 Accuracy:  0.3758\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:  1.71834 Accuracy:  0.3748\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:  1.51176 Accuracy:  0.3706\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:  1.54268 Accuracy:  0.383\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:  1.6599 Accuracy:  0.3908\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  1.83689 Accuracy:  0.3914\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:  1.6616 Accuracy:  0.3842\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:  1.47771 Accuracy:  0.3712\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:  1.53663 Accuracy:  0.3864\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:  1.68241 Accuracy:  0.3798\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  1.79615 Accuracy:  0.3998\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:  1.65268 Accuracy:  0.3826\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:  1.46195 Accuracy:  0.3972\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:  1.5276 Accuracy:  0.3878\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:  1.69323 Accuracy:  0.396\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  1.81893 Accuracy:  0.4074\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:  1.65073 Accuracy:  0.4012\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:  1.43958 Accuracy:  0.3952\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:  1.50484 Accuracy:  0.4002\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:  1.65816 Accuracy:  0.3962\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  1.84251 Accuracy:  0.3848\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:  1.61077 Accuracy:  0.3892\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:  1.41878 Accuracy:  0.4102\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:  1.48439 Accuracy:  0.4098\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:  1.61207 Accuracy:  0.4088\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  1.78672 Accuracy:  0.4166\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:  1.54793 Accuracy:  0.3904\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:  1.39421 Accuracy:  0.425\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:  1.45697 Accuracy:  0.4146\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:  1.62009 Accuracy:  0.4074\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  1.81335 Accuracy:  0.4216\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:  1.55392 Accuracy:  0.4106\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:  1.40432 Accuracy:  0.4054\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:  1.49542 Accuracy:  0.4354\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:  1.58776 Accuracy:  0.4146\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  1.74968 Accuracy:  0.4374\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:  1.53669 Accuracy:  0.4066\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:  1.3995 Accuracy:  0.4266\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:  1.45022 Accuracy:  0.4304\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:  1.57155 Accuracy:  0.4284\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  1.79493 Accuracy:  0.4446\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:  1.50053 Accuracy:  0.4168\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:  1.39126 Accuracy:  0.439\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:  1.45349 Accuracy:  0.4338\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:  1.55595 Accuracy:  0.4378\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  1.79021 Accuracy:  0.4348\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:  1.44548 Accuracy:  0.4374\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:  1.36167 Accuracy:  0.4444\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:  1.4668 Accuracy:  0.446\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:  1.51319 Accuracy:  0.456\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  1.71363 Accuracy:  0.4412\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:  1.48097 Accuracy:  0.4486\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:  1.29523 Accuracy:  0.4356\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:  1.40757 Accuracy:  0.4572\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:  1.49533 Accuracy:  0.471\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  1.71941 Accuracy:  0.4434\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:  1.44412 Accuracy:  0.4304\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:  1.32209 Accuracy:  0.452\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:  1.44283 Accuracy:  0.4592\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:  1.46868 Accuracy:  0.461\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  1.63607 Accuracy:  0.4788\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:  1.35253 Accuracy:  0.4498\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:  1.27815 Accuracy:  0.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, CIFAR-10 Batch 4:  Loss:  1.38575 Accuracy:  0.4674\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:  1.44991 Accuracy:  0.4538\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  1.61367 Accuracy:  0.4782\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:  1.39778 Accuracy:  0.4594\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:  1.29948 Accuracy:  0.4664\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:  1.32155 Accuracy:  0.4854\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:  1.41859 Accuracy:  0.4626\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  1.60005 Accuracy:  0.4766\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:  1.38735 Accuracy:  0.4726\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:  1.23365 Accuracy:  0.4928\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:  1.36032 Accuracy:  0.4924\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:  1.40295 Accuracy:  0.4656\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  1.60233 Accuracy:  0.4944\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:  1.32951 Accuracy:  0.4948\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:  1.22326 Accuracy:  0.4724\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:  1.36723 Accuracy:  0.4814\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:  1.36361 Accuracy:  0.4896\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  1.56746 Accuracy:  0.4932\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:  1.33702 Accuracy:  0.4878\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:  1.12748 Accuracy:  0.4926\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:  1.35859 Accuracy:  0.471\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:  1.35546 Accuracy:  0.481\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  1.51598 Accuracy:  0.4898\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:  1.29207 Accuracy:  0.5032\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:  1.15723 Accuracy:  0.4876\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:  1.35275 Accuracy:  0.506\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:  1.34393 Accuracy:  0.4926\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  1.44821 Accuracy:  0.5068\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:  1.29046 Accuracy:  0.512\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:  1.11135 Accuracy:  0.5106\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:  1.30452 Accuracy:  0.5008\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:  1.32752 Accuracy:  0.492\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  1.41746 Accuracy:  0.501\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:  1.22282 Accuracy:  0.5184\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:  1.12451 Accuracy:  0.5056\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:  1.30282 Accuracy:  0.5212\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:  1.30292 Accuracy:  0.4898\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  1.45702 Accuracy:  0.5122\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:  1.20186 Accuracy:  0.5176\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:  1.05084 Accuracy:  0.5266\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:  1.30604 Accuracy:  0.5196\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:  1.30002 Accuracy:  0.5118\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  1.35638 Accuracy:  0.5224\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:  1.20716 Accuracy:  0.5242\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:  1.03493 Accuracy:  0.528\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:  1.28295 Accuracy:  0.4918\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:  1.27203 Accuracy:  0.5052\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  1.40787 Accuracy:  0.521\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:  1.20566 Accuracy:  0.54\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:  1.05371 Accuracy:  0.5228\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:  1.28412 Accuracy:  0.5152\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:  1.33296 Accuracy:  0.4868\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  1.34737 Accuracy:  0.5258\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:  1.16847 Accuracy:  0.5502\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:  1.00068 Accuracy:  0.5394\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:  1.24369 Accuracy:  0.5308\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:  1.19617 Accuracy:  0.5446\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  1.30534 Accuracy:  0.5202\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:  1.23751 Accuracy:  0.5438\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:  0.997953 Accuracy:  0.5412\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:  1.22611 Accuracy:  0.5406\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:  1.20058 Accuracy:  0.5116\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  1.27035 Accuracy:  0.5498\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:  1.14324 Accuracy:  0.5346\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:  0.987942 Accuracy:  0.5368\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:  1.19465 Accuracy:  0.541\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:  1.15987 Accuracy:  0.5308\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  1.25979 Accuracy:  0.5402\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:  1.12222 Accuracy:  0.5554\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:  0.986566 Accuracy:  0.5432\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:  1.23052 Accuracy:  0.5408\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:  1.14945 Accuracy:  0.5358\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  1.21076 Accuracy:  0.5458\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:  1.1266 Accuracy:  0.5512\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:  0.9465 Accuracy:  0.544\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:  1.2264 Accuracy:  0.5578\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:  1.15833 Accuracy:  0.5438\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  1.21534 Accuracy:  0.5532\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:  1.09144 Accuracy:  0.5548\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:  0.945605 Accuracy:  0.5632\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:  1.20387 Accuracy:  0.5514\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:  1.16918 Accuracy:  0.5302\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  1.1787 Accuracy:  0.5652\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:  1.12333 Accuracy:  0.5654\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:  0.914936 Accuracy:  0.56\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:  1.17431 Accuracy:  0.5592\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:  1.11236 Accuracy:  0.5556\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  1.23229 Accuracy:  0.554\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:  0.992951 Accuracy:  0.5584\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:  0.903917 Accuracy:  0.552\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:  1.17469 Accuracy:  0.568\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:  1.05223 Accuracy:  0.5536\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  1.1501 Accuracy:  0.5536\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:  1.0199 Accuracy:  0.5608\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:  0.884004 Accuracy:  0.556\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:  1.18857 Accuracy:  0.5458\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:  1.10162 Accuracy:  0.553\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  1.13328 Accuracy:  0.5506\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:  1.0196 Accuracy:  0.563\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:  0.900225 Accuracy:  0.559\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:  1.14298 Accuracy:  0.5578\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:  1.08931 Accuracy:  0.5666\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  1.11172 Accuracy:  0.567\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:  1.02029 Accuracy:  0.5684\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:  0.856851 Accuracy:  0.571\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:  1.12671 Accuracy:  0.571\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:  1.04045 Accuracy:  0.5656\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  1.09791 Accuracy:  0.5562\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:  0.973725 Accuracy:  0.5824\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:  0.819979 Accuracy:  0.5724\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:  1.07266 Accuracy:  0.5796\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:  1.00009 Accuracy:  0.5774\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  1.10021 Accuracy:  0.5786\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:  0.969304 Accuracy:  0.576\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:  0.825759 Accuracy:  0.567\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:  1.06046 Accuracy:  0.581\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:  0.998124 Accuracy:  0.5746\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  1.0509 Accuracy:  0.5674\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:  0.93962 Accuracy:  0.575\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:  0.899772 Accuracy:  0.5718\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:  1.064 Accuracy:  0.5712\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:  0.973441 Accuracy:  0.5784\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  1.07998 Accuracy:  0.5866\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:  0.987868 Accuracy:  0.5662\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:  0.778168 Accuracy:  0.572\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:  1.07403 Accuracy:  0.5866\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:  0.998121 Accuracy:  0.5676\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  1.04004 Accuracy:  0.58\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:  0.934316 Accuracy:  0.5854\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:  0.79306 Accuracy:  0.5802\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:  1.04989 Accuracy:  0.5764\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:  1.0159 Accuracy:  0.5638\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  1.05856 Accuracy:  0.5786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, CIFAR-10 Batch 2:  Loss:  0.907216 Accuracy:  0.5868\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:  0.77819 Accuracy:  0.5768\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:  1.05099 Accuracy:  0.5802\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:  0.968666 Accuracy:  0.5694\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  1.01582 Accuracy:  0.586\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:  0.921005 Accuracy:  0.591\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:  0.783417 Accuracy:  0.5976\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:  1.11793 Accuracy:  0.5526\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:  0.959995 Accuracy:  0.5902\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  1.01425 Accuracy:  0.5888\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:  0.952394 Accuracy:  0.5836\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:  0.780906 Accuracy:  0.5894\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:  1.03708 Accuracy:  0.5846\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:  0.99995 Accuracy:  0.5628\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  0.994545 Accuracy:  0.5938\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:  0.895785 Accuracy:  0.59\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:  0.825009 Accuracy:  0.566\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:  0.999185 Accuracy:  0.5792\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:  1.0043 Accuracy:  0.5828\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  0.988513 Accuracy:  0.5934\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:  0.922178 Accuracy:  0.5854\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:  0.797391 Accuracy:  0.5832\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:  0.982436 Accuracy:  0.5842\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:  0.894306 Accuracy:  0.587\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  0.957094 Accuracy:  0.6062\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:  0.908664 Accuracy:  0.599\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:  0.721606 Accuracy:  0.5876\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:  0.995924 Accuracy:  0.596\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:  0.947514 Accuracy:  0.6034\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  0.961331 Accuracy:  0.594\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:  0.884494 Accuracy:  0.5968\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:  0.759364 Accuracy:  0.583\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:  0.989157 Accuracy:  0.589\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:  0.971089 Accuracy:  0.5826\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  1.01492 Accuracy:  0.6008\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:  0.841254 Accuracy:  0.599\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:  0.733541 Accuracy:  0.5962\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:  0.980222 Accuracy:  0.5812\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:  0.90461 Accuracy:  0.5832\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  0.918328 Accuracy:  0.601\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:  0.862526 Accuracy:  0.6072\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:  0.73282 Accuracy:  0.6014\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:  0.958912 Accuracy:  0.5852\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:  0.922598 Accuracy:  0.5882\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  0.915307 Accuracy:  0.5886\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:  0.834763 Accuracy:  0.5806\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:  0.697555 Accuracy:  0.5918\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:  0.962175 Accuracy:  0.5908\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:  0.880911 Accuracy:  0.5982\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  0.951468 Accuracy:  0.5996\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:  0.805885 Accuracy:  0.6036\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:  0.729443 Accuracy:  0.597\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:  0.906903 Accuracy:  0.5974\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:  0.835309 Accuracy:  0.6126\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  0.941985 Accuracy:  0.6042\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:  0.801588 Accuracy:  0.5978\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:  0.697128 Accuracy:  0.599\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:  0.9258 Accuracy:  0.6012\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:  0.926853 Accuracy:  0.5818\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  0.940775 Accuracy:  0.5956\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:  0.823386 Accuracy:  0.6016\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:  0.687087 Accuracy:  0.6\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:  0.904493 Accuracy:  0.5858\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:  0.872733 Accuracy:  0.5918\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  0.928482 Accuracy:  0.6022\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:  0.796941 Accuracy:  0.6116\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:  0.694257 Accuracy:  0.5928\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:  0.893796 Accuracy:  0.617\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:  0.859767 Accuracy:  0.616\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  0.956386 Accuracy:  0.5894\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:  0.82181 Accuracy:  0.6006\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:  0.671949 Accuracy:  0.6012\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:  0.907355 Accuracy:  0.602\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:  0.84522 Accuracy:  0.6046\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  0.872032 Accuracy:  0.6096\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:  0.776256 Accuracy:  0.603\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:  0.678075 Accuracy:  0.601\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:  0.901669 Accuracy:  0.5994\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:  0.894152 Accuracy:  0.5976\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  0.89313 Accuracy:  0.6066\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:  0.785103 Accuracy:  0.6104\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:  0.627552 Accuracy:  0.6048\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:  0.884842 Accuracy:  0.6102\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:  0.856265 Accuracy:  0.6134\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  0.886791 Accuracy:  0.6138\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:  0.74852 Accuracy:  0.6004\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:  0.654872 Accuracy:  0.6066\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:  0.854339 Accuracy:  0.6136\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:  0.88248 Accuracy:  0.5948\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  0.853201 Accuracy:  0.6142\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:  0.722899 Accuracy:  0.6034\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:  0.663501 Accuracy:  0.6112\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:  0.867086 Accuracy:  0.611\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:  0.809497 Accuracy:  0.6062\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  0.888335 Accuracy:  0.6064\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:  0.759852 Accuracy:  0.6024\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:  0.645761 Accuracy:  0.6208\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:  0.824203 Accuracy:  0.6086\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:  0.801013 Accuracy:  0.6124\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  0.869294 Accuracy:  0.6014\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:  0.830676 Accuracy:  0.608\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:  0.645051 Accuracy:  0.6164\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:  0.831634 Accuracy:  0.6194\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:  0.811772 Accuracy:  0.6042\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  0.890598 Accuracy:  0.61\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:  0.752637 Accuracy:  0.6118\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:  0.636048 Accuracy:  0.6092\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:  0.851573 Accuracy:  0.597\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:  0.806936 Accuracy:  0.5908\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  0.808516 Accuracy:  0.6184\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:  0.726511 Accuracy:  0.6098\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:  0.636716 Accuracy:  0.6024\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:  0.802184 Accuracy:  0.6216\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:  0.774882 Accuracy:  0.6088\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  0.794299 Accuracy:  0.627\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:  0.750791 Accuracy:  0.6192\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:  0.625814 Accuracy:  0.615\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:  0.83847 Accuracy:  0.6106\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:  0.780569 Accuracy:  0.6076\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  0.807177 Accuracy:  0.6172\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:  0.734897 Accuracy:  0.6136\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:  0.61578 Accuracy:  0.617\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:  0.836499 Accuracy:  0.6034\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:  0.81335 Accuracy:  0.603\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  0.805849 Accuracy:  0.609\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:  0.765818 Accuracy:  0.6164\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:  0.60928 Accuracy:  0.608\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:  0.806069 Accuracy:  0.6188\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:  0.746056 Accuracy:  0.6128\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  0.787833 Accuracy:  0.6158\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:  0.704463 Accuracy:  0.6208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, CIFAR-10 Batch 3:  Loss:  0.593234 Accuracy:  0.624\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:  0.773718 Accuracy:  0.618\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:  0.763771 Accuracy:  0.6208\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  0.782322 Accuracy:  0.6158\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:  0.733394 Accuracy:  0.6042\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:  0.628434 Accuracy:  0.6144\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:  0.804591 Accuracy:  0.611\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:  0.766199 Accuracy:  0.5932\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  0.823805 Accuracy:  0.6106\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:  0.721574 Accuracy:  0.6178\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:  0.59795 Accuracy:  0.6038\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:  0.809557 Accuracy:  0.6064\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:  0.715672 Accuracy:  0.6168\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  0.772074 Accuracy:  0.6252\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:  0.704746 Accuracy:  0.6188\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:  0.613132 Accuracy:  0.615\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:  0.786438 Accuracy:  0.6138\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:  0.762024 Accuracy:  0.6186\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  0.727491 Accuracy:  0.6126\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:  0.68552 Accuracy:  0.6188\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:  0.582181 Accuracy:  0.6242\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:  0.760579 Accuracy:  0.6096\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:  0.811904 Accuracy:  0.5806\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  0.738481 Accuracy:  0.6152\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:  0.73754 Accuracy:  0.6194\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:  0.578355 Accuracy:  0.618\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:  0.744942 Accuracy:  0.6104\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:  0.779399 Accuracy:  0.6144\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  0.736842 Accuracy:  0.6182\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:  0.686716 Accuracy:  0.6146\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:  0.571531 Accuracy:  0.622\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:  0.740727 Accuracy:  0.6178\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:  0.756143 Accuracy:  0.5898\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  0.791402 Accuracy:  0.6142\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:  0.676596 Accuracy:  0.623\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:  0.592864 Accuracy:  0.624\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:  0.769905 Accuracy:  0.6028\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:  0.703501 Accuracy:  0.61\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  0.724879 Accuracy:  0.6212\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:  0.649419 Accuracy:  0.6156\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:  0.543631 Accuracy:  0.6152\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:  0.731579 Accuracy:  0.6214\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:  0.697504 Accuracy:  0.6206\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  0.773436 Accuracy:  0.6008\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:  0.650917 Accuracy:  0.6192\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:  0.533896 Accuracy:  0.6218\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:  0.705925 Accuracy:  0.6212\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:  0.729295 Accuracy:  0.6178\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  0.703843 Accuracy:  0.6182\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:  0.665088 Accuracy:  0.6156\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:  0.585393 Accuracy:  0.6288\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:  0.725182 Accuracy:  0.6184\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:  0.709423 Accuracy:  0.5918\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  0.752223 Accuracy:  0.6214\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:  0.65856 Accuracy:  0.6122\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:  0.520381 Accuracy:  0.6268\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:  0.703918 Accuracy:  0.6228\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:  0.738309 Accuracy:  0.6022\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  0.723191 Accuracy:  0.6252\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:  0.660731 Accuracy:  0.6314\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:  0.519268 Accuracy:  0.6204\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:  0.696747 Accuracy:  0.6154\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:  0.677351 Accuracy:  0.6066\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  0.704071 Accuracy:  0.6248\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:  0.627873 Accuracy:  0.624\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:  0.536784 Accuracy:  0.6198\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:  0.714067 Accuracy:  0.6194\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:  0.687832 Accuracy:  0.6228\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  0.697677 Accuracy:  0.6208\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:  0.620387 Accuracy:  0.6264\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:  0.542742 Accuracy:  0.616\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:  0.667254 Accuracy:  0.6158\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:  0.653882 Accuracy:  0.6248\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  0.699075 Accuracy:  0.6216\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:  0.650323 Accuracy:  0.6162\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:  0.558895 Accuracy:  0.6344\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:  0.683859 Accuracy:  0.6188\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:  0.641071 Accuracy:  0.6114\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  0.701868 Accuracy:  0.6124\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:  0.6105 Accuracy:  0.6178\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:  0.516693 Accuracy:  0.6292\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:  0.707734 Accuracy:  0.6116\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:  0.68057 Accuracy:  0.6142\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  0.676537 Accuracy:  0.624\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:  0.598754 Accuracy:  0.62\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:  0.529354 Accuracy:  0.6296\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:  0.683897 Accuracy:  0.6176\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:  0.689593 Accuracy:  0.6124\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  0.679864 Accuracy:  0.6262\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:  0.578614 Accuracy:  0.6228\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:  0.520592 Accuracy:  0.628\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:  0.663983 Accuracy:  0.6252\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:  0.643086 Accuracy:  0.6266\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  0.65428 Accuracy:  0.6238\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:  0.600471 Accuracy:  0.6188\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:  0.518158 Accuracy:  0.6262\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:  0.682688 Accuracy:  0.6248\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:  0.650105 Accuracy:  0.6108\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  0.690303 Accuracy:  0.6154\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:  0.573545 Accuracy:  0.6272\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:  0.525115 Accuracy:  0.6226\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:  0.6616 Accuracy:  0.616\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:  0.621045 Accuracy:  0.6266\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  0.660239 Accuracy:  0.635\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:  0.573137 Accuracy:  0.6286\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:  0.50121 Accuracy:  0.6272\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:  0.652734 Accuracy:  0.6214\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:  0.655622 Accuracy:  0.6192\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  0.687964 Accuracy:  0.6248\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:  0.609303 Accuracy:  0.6272\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:  0.508481 Accuracy:  0.6148\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:  0.656861 Accuracy:  0.6292\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:  0.665867 Accuracy:  0.6172\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  0.666021 Accuracy:  0.6326\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:  0.625341 Accuracy:  0.621\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:  0.487885 Accuracy:  0.633\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:  0.626247 Accuracy:  0.6238\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:  0.677138 Accuracy:  0.6134\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  0.660891 Accuracy:  0.618\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:  0.589002 Accuracy:  0.627\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:  0.50767 Accuracy:  0.6202\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:  0.647817 Accuracy:  0.62\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:  0.599375 Accuracy:  0.624\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  0.651297 Accuracy:  0.63\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:  0.569767 Accuracy:  0.6244\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:  0.467756 Accuracy:  0.6228\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:  0.637692 Accuracy:  0.6262\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:  0.618301 Accuracy:  0.6134\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  0.633665 Accuracy:  0.6256\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:  0.574953 Accuracy:  0.6234\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:  0.462325 Accuracy:  0.626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106, CIFAR-10 Batch 4:  Loss:  0.6248 Accuracy:  0.6212\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:  0.581441 Accuracy:  0.6206\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  0.654932 Accuracy:  0.6164\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:  0.549161 Accuracy:  0.6202\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:  0.485644 Accuracy:  0.6276\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:  0.579549 Accuracy:  0.6238\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:  0.579959 Accuracy:  0.614\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  0.675554 Accuracy:  0.6208\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:  0.525134 Accuracy:  0.634\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:  0.512795 Accuracy:  0.6204\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:  0.604498 Accuracy:  0.628\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:  0.633324 Accuracy:  0.613\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  0.634674 Accuracy:  0.6096\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:  0.547529 Accuracy:  0.6216\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:  0.452063 Accuracy:  0.6286\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:  0.592052 Accuracy:  0.6236\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:  0.594856 Accuracy:  0.626\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  0.627822 Accuracy:  0.6212\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:  0.556838 Accuracy:  0.6248\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:  0.454007 Accuracy:  0.6318\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:  0.599674 Accuracy:  0.6188\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:  0.573735 Accuracy:  0.619\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  0.568992 Accuracy:  0.6308\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:  0.563221 Accuracy:  0.6234\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:  0.482829 Accuracy:  0.621\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:  0.599117 Accuracy:  0.626\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:  0.56649 Accuracy:  0.6326\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  0.605982 Accuracy:  0.6326\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:  0.528718 Accuracy:  0.6294\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:  0.452456 Accuracy:  0.6288\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:  0.585512 Accuracy:  0.634\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:  0.564886 Accuracy:  0.6192\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  0.595 Accuracy:  0.6172\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:  0.508227 Accuracy:  0.6254\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:  0.482345 Accuracy:  0.636\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:  0.608107 Accuracy:  0.6196\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:  0.540705 Accuracy:  0.6186\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  0.604842 Accuracy:  0.625\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:  0.507688 Accuracy:  0.6284\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:  0.431677 Accuracy:  0.631\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:  0.558322 Accuracy:  0.632\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:  0.541812 Accuracy:  0.6312\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  0.578418 Accuracy:  0.6304\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:  0.516256 Accuracy:  0.63\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:  0.44838 Accuracy:  0.6358\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:  0.533147 Accuracy:  0.6278\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:  0.553118 Accuracy:  0.6246\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  0.579796 Accuracy:  0.6278\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:  0.524902 Accuracy:  0.6246\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:  0.431223 Accuracy:  0.632\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:  0.537607 Accuracy:  0.6288\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:  0.541378 Accuracy:  0.6264\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  0.558347 Accuracy:  0.6336\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:  0.528549 Accuracy:  0.6314\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:  0.445065 Accuracy:  0.6334\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:  0.567882 Accuracy:  0.6244\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:  0.536433 Accuracy:  0.6258\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  0.55956 Accuracy:  0.6252\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:  0.517843 Accuracy:  0.6324\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:  0.427367 Accuracy:  0.6354\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:  0.530813 Accuracy:  0.6338\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:  0.531363 Accuracy:  0.6278\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  0.548222 Accuracy:  0.6338\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:  0.499126 Accuracy:  0.618\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:  0.401357 Accuracy:  0.6304\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:  0.548261 Accuracy:  0.6208\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:  0.591183 Accuracy:  0.6164\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  0.583851 Accuracy:  0.6186\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:  0.499183 Accuracy:  0.6246\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:  0.443436 Accuracy:  0.623\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:  0.499282 Accuracy:  0.623\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:  0.540069 Accuracy:  0.6354\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  0.595998 Accuracy:  0.63\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:  0.524992 Accuracy:  0.622\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:  0.443094 Accuracy:  0.6388\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:  0.533217 Accuracy:  0.6298\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:  0.569263 Accuracy:  0.6244\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  0.558627 Accuracy:  0.634\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:  0.500929 Accuracy:  0.635\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:  0.434968 Accuracy:  0.6302\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:  0.508088 Accuracy:  0.6342\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:  0.513645 Accuracy:  0.62\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  0.595685 Accuracy:  0.632\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:  0.492786 Accuracy:  0.6328\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:  0.405739 Accuracy:  0.6304\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:  0.490859 Accuracy:  0.6246\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:  0.578805 Accuracy:  0.6156\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  0.586445 Accuracy:  0.6196\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:  0.485639 Accuracy:  0.6358\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:  0.402249 Accuracy:  0.629\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:  0.528716 Accuracy:  0.6214\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:  0.545581 Accuracy:  0.6332\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  0.524398 Accuracy:  0.6226\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:  0.495613 Accuracy:  0.625\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:  0.448132 Accuracy:  0.632\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:  0.526073 Accuracy:  0.6272\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:  0.570244 Accuracy:  0.6066\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  0.54387 Accuracy:  0.6374\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:  0.492196 Accuracy:  0.6276\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:  0.413918 Accuracy:  0.6314\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:  0.515764 Accuracy:  0.627\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:  0.537314 Accuracy:  0.6058\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  0.566377 Accuracy:  0.6162\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:  0.467764 Accuracy:  0.6306\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:  0.376201 Accuracy:  0.6314\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:  0.533315 Accuracy:  0.6272\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:  0.559665 Accuracy:  0.613\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  0.515159 Accuracy:  0.6302\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:  0.497392 Accuracy:  0.6298\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:  0.385454 Accuracy:  0.6266\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:  0.521332 Accuracy:  0.6214\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:  0.524524 Accuracy:  0.602\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  0.546304 Accuracy:  0.6274\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:  0.479624 Accuracy:  0.6344\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:  0.399372 Accuracy:  0.6368\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:  0.499079 Accuracy:  0.6318\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:  0.52919 Accuracy:  0.6216\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  0.521518 Accuracy:  0.6214\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:  0.487154 Accuracy:  0.6316\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:  0.38128 Accuracy:  0.6332\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:  0.483573 Accuracy:  0.6356\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:  0.498777 Accuracy:  0.6124\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  0.530984 Accuracy:  0.64\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:  0.462508 Accuracy:  0.628\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:  0.388656 Accuracy:  0.6408\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:  0.454003 Accuracy:  0.6366\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:  0.524843 Accuracy:  0.6182\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  0.525339 Accuracy:  0.634\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:  0.46883 Accuracy:  0.6338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132, CIFAR-10 Batch 3:  Loss:  0.375046 Accuracy:  0.6226\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:  0.48674 Accuracy:  0.6338\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:  0.498892 Accuracy:  0.6302\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  0.518865 Accuracy:  0.6316\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:  0.431279 Accuracy:  0.6406\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:  0.381698 Accuracy:  0.6394\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:  0.491649 Accuracy:  0.6252\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:  0.465872 Accuracy:  0.6362\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  0.504797 Accuracy:  0.637\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:  0.434529 Accuracy:  0.6302\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:  0.398492 Accuracy:  0.6294\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:  0.483582 Accuracy:  0.6334\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:  0.547055 Accuracy:  0.6066\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  0.517484 Accuracy:  0.6344\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:  0.459214 Accuracy:  0.6246\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:  0.383007 Accuracy:  0.634\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:  0.460031 Accuracy:  0.6314\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:  0.509559 Accuracy:  0.6294\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  0.526203 Accuracy:  0.6292\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:  0.430696 Accuracy:  0.6314\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:  0.378255 Accuracy:  0.6322\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:  0.477391 Accuracy:  0.626\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:  0.46595 Accuracy:  0.639\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  0.502118 Accuracy:  0.6332\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:  0.461697 Accuracy:  0.6226\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:  0.360454 Accuracy:  0.6318\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:  0.473519 Accuracy:  0.6312\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:  0.515731 Accuracy:  0.616\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  0.512362 Accuracy:  0.626\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:  0.432399 Accuracy:  0.6338\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:  0.355332 Accuracy:  0.6324\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:  0.466362 Accuracy:  0.6252\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:  0.543586 Accuracy:  0.597\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  0.49075 Accuracy:  0.632\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:  0.443656 Accuracy:  0.6354\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:  0.323664 Accuracy:  0.6288\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:  0.434589 Accuracy:  0.6306\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:  0.464504 Accuracy:  0.6326\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  0.492974 Accuracy:  0.63\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:  0.426063 Accuracy:  0.6296\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:  0.405505 Accuracy:  0.6122\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:  0.447987 Accuracy:  0.6254\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:  0.472491 Accuracy:  0.6246\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  0.453223 Accuracy:  0.635\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:  0.423753 Accuracy:  0.6314\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:  0.329585 Accuracy:  0.633\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:  0.470818 Accuracy:  0.6344\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:  0.461607 Accuracy:  0.617\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  0.443953 Accuracy:  0.6224\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:  0.424502 Accuracy:  0.6292\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:  0.358815 Accuracy:  0.6294\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:  0.440506 Accuracy:  0.6274\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:  0.48401 Accuracy:  0.6188\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  0.462885 Accuracy:  0.6154\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:  0.425407 Accuracy:  0.6282\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:  0.378244 Accuracy:  0.6354\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:  0.458979 Accuracy:  0.6404\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:  0.469762 Accuracy:  0.6228\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  0.462744 Accuracy:  0.6264\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:  0.425501 Accuracy:  0.6342\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:  0.335793 Accuracy:  0.6344\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:  0.439461 Accuracy:  0.6332\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:  0.458406 Accuracy:  0.6238\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  0.45199 Accuracy:  0.6322\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:  0.435781 Accuracy:  0.636\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:  0.347307 Accuracy:  0.6378\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:  0.430591 Accuracy:  0.6316\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:  0.444306 Accuracy:  0.6354\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  0.44429 Accuracy:  0.6342\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:  0.448265 Accuracy:  0.631\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:  0.35518 Accuracy:  0.6438\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:  0.435888 Accuracy:  0.632\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:  0.470041 Accuracy:  0.629\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  0.475682 Accuracy:  0.6366\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:  0.424891 Accuracy:  0.6258\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:  0.368083 Accuracy:  0.6346\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:  0.437 Accuracy:  0.6342\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:  0.462094 Accuracy:  0.6192\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  0.439559 Accuracy:  0.6294\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:  0.421831 Accuracy:  0.6258\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:  0.340428 Accuracy:  0.6336\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:  0.424603 Accuracy:  0.635\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:  0.43483 Accuracy:  0.63\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  0.474664 Accuracy:  0.6306\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:  0.430878 Accuracy:  0.6324\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:  0.348505 Accuracy:  0.6326\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:  0.42716 Accuracy:  0.6302\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:  0.447861 Accuracy:  0.6294\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  0.455311 Accuracy:  0.6298\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:  0.442513 Accuracy:  0.6282\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:  0.34261 Accuracy:  0.6374\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:  0.44701 Accuracy:  0.626\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:  0.405088 Accuracy:  0.6322\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  0.444932 Accuracy:  0.6364\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:  0.398205 Accuracy:  0.6314\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:  0.332931 Accuracy:  0.6302\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:  0.413757 Accuracy:  0.6292\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:  0.422684 Accuracy:  0.6218\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  0.464677 Accuracy:  0.6284\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:  0.39996 Accuracy:  0.6284\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:  0.335797 Accuracy:  0.6302\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:  0.418851 Accuracy:  0.634\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:  0.418236 Accuracy:  0.622\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  0.447921 Accuracy:  0.6392\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:  0.408613 Accuracy:  0.6366\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:  0.333648 Accuracy:  0.6298\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:  0.412075 Accuracy:  0.6338\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:  0.41585 Accuracy:  0.6304\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  0.454131 Accuracy:  0.6338\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:  0.383744 Accuracy:  0.6338\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:  0.337612 Accuracy:  0.6268\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:  0.422471 Accuracy:  0.6236\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:  0.448977 Accuracy:  0.6184\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  0.467738 Accuracy:  0.6388\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:  0.410159 Accuracy:  0.6252\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:  0.313714 Accuracy:  0.6324\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:  0.416447 Accuracy:  0.6392\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:  0.457287 Accuracy:  0.6166\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  0.434018 Accuracy:  0.6314\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:  0.385363 Accuracy:  0.6386\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:  0.320246 Accuracy:  0.6378\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:  0.423159 Accuracy:  0.6258\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:  0.461304 Accuracy:  0.6172\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  0.415094 Accuracy:  0.6332\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:  0.379593 Accuracy:  0.631\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:  0.330292 Accuracy:  0.6314\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:  0.415601 Accuracy:  0.629\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:  0.398965 Accuracy:  0.623\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  0.405001 Accuracy:  0.6342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, CIFAR-10 Batch 2:  Loss:  0.371246 Accuracy:  0.6322\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:  0.324987 Accuracy:  0.6364\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:  0.431707 Accuracy:  0.6298\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:  0.442245 Accuracy:  0.611\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  0.451753 Accuracy:  0.637\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:  0.375551 Accuracy:  0.6326\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:  0.333409 Accuracy:  0.6372\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:  0.385398 Accuracy:  0.6304\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:  0.398795 Accuracy:  0.6042\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  0.409788 Accuracy:  0.6252\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:  0.389605 Accuracy:  0.6318\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:  0.298325 Accuracy:  0.6348\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:  0.388055 Accuracy:  0.627\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:  0.410225 Accuracy:  0.6328\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  0.40523 Accuracy:  0.6306\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:  0.386588 Accuracy:  0.6242\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:  0.315622 Accuracy:  0.6284\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:  0.380289 Accuracy:  0.6348\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:  0.409765 Accuracy:  0.6286\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  0.391239 Accuracy:  0.6256\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:  0.377472 Accuracy:  0.6328\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:  0.306826 Accuracy:  0.6342\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:  0.407357 Accuracy:  0.6336\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:  0.398429 Accuracy:  0.6356\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  0.381492 Accuracy:  0.6328\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:  0.366017 Accuracy:  0.6236\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:  0.30954 Accuracy:  0.6366\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:  0.42374 Accuracy:  0.6218\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:  0.42276 Accuracy:  0.6228\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.44259 Accuracy:  0.6252\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:  0.377417 Accuracy:  0.6262\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:  0.274647 Accuracy:  0.6354\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:  0.389625 Accuracy:  0.626\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:  0.401249 Accuracy:  0.6292\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.387577 Accuracy:  0.6328\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:  0.354822 Accuracy:  0.6372\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:  0.299315 Accuracy:  0.6284\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:  0.405463 Accuracy:  0.627\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:  0.402574 Accuracy:  0.6264\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  0.406657 Accuracy:  0.6218\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:  0.353928 Accuracy:  0.6378\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:  0.309181 Accuracy:  0.624\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:  0.38439 Accuracy:  0.6356\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:  0.434469 Accuracy:  0.6124\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.388137 Accuracy:  0.63\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:  0.386302 Accuracy:  0.6266\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:  0.307555 Accuracy:  0.6312\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:  0.377347 Accuracy:  0.6298\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:  0.420888 Accuracy:  0.6094\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.414362 Accuracy:  0.6388\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:  0.355743 Accuracy:  0.629\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:  0.28351 Accuracy:  0.6412\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:  0.375021 Accuracy:  0.6338\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:  0.385794 Accuracy:  0.6398\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.37129 Accuracy:  0.6318\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:  0.338649 Accuracy:  0.6346\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:  0.303922 Accuracy:  0.6404\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:  0.381428 Accuracy:  0.6362\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:  0.430227 Accuracy:  0.606\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.373657 Accuracy:  0.6344\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:  0.347705 Accuracy:  0.6318\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:  0.2787 Accuracy:  0.626\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:  0.385434 Accuracy:  0.6292\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:  0.399639 Accuracy:  0.6196\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.391894 Accuracy:  0.6354\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:  0.339595 Accuracy:  0.6314\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:  0.293249 Accuracy:  0.6374\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:  0.379905 Accuracy:  0.6226\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:  0.39113 Accuracy:  0.626\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.371681 Accuracy:  0.6368\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:  0.361989 Accuracy:  0.6334\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:  0.286904 Accuracy:  0.628\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:  0.389855 Accuracy:  0.6178\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:  0.400597 Accuracy:  0.6128\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.383248 Accuracy:  0.6312\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:  0.340869 Accuracy:  0.6304\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:  0.296332 Accuracy:  0.6386\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:  0.35383 Accuracy:  0.6376\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:  0.399198 Accuracy:  0.6142\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.417946 Accuracy:  0.6226\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:  0.332864 Accuracy:  0.6324\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:  0.269198 Accuracy:  0.6344\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:  0.360488 Accuracy:  0.6358\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:  0.384411 Accuracy:  0.6306\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.40952 Accuracy:  0.625\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:  0.331561 Accuracy:  0.6342\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:  0.258942 Accuracy:  0.6252\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:  0.367682 Accuracy:  0.6276\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:  0.372124 Accuracy:  0.6106\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.368201 Accuracy:  0.6374\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:  0.32494 Accuracy:  0.635\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:  0.277472 Accuracy:  0.635\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:  0.367731 Accuracy:  0.6298\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:  0.382295 Accuracy:  0.6242\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.393267 Accuracy:  0.6366\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:  0.317837 Accuracy:  0.6384\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:  0.293669 Accuracy:  0.6192\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:  0.384257 Accuracy:  0.6328\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:  0.378365 Accuracy:  0.6204\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.400778 Accuracy:  0.631\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:  0.310573 Accuracy:  0.6372\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:  0.300216 Accuracy:  0.6292\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:  0.380011 Accuracy:  0.6196\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:  0.39117 Accuracy:  0.6208\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.389903 Accuracy:  0.6348\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:  0.326268 Accuracy:  0.6332\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:  0.266101 Accuracy:  0.631\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:  0.362779 Accuracy:  0.6238\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:  0.397084 Accuracy:  0.6172\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.367397 Accuracy:  0.6354\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:  0.32226 Accuracy:  0.6282\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:  0.273918 Accuracy:  0.6274\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:  0.385363 Accuracy:  0.6242\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:  0.359703 Accuracy:  0.6228\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.356919 Accuracy:  0.6268\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:  0.307484 Accuracy:  0.6354\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:  0.252727 Accuracy:  0.6304\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:  0.356153 Accuracy:  0.6308\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:  0.355115 Accuracy:  0.6228\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.35652 Accuracy:  0.6296\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:  0.320301 Accuracy:  0.6306\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:  0.254302 Accuracy:  0.6304\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:  0.345994 Accuracy:  0.6312\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:  0.368458 Accuracy:  0.6296\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.3567 Accuracy:  0.6296\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:  0.302027 Accuracy:  0.6328\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:  0.268582 Accuracy:  0.6348\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:  0.338145 Accuracy:  0.632\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:  0.33565 Accuracy:  0.6236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.344037 Accuracy:  0.6284\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:  0.304156 Accuracy:  0.6336\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:  0.252749 Accuracy:  0.6264\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:  0.352166 Accuracy:  0.6186\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:  0.36783 Accuracy:  0.6314\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.394068 Accuracy:  0.627\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:  0.306657 Accuracy:  0.6228\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:  0.267203 Accuracy:  0.6352\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:  0.34173 Accuracy:  0.6258\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:  0.369636 Accuracy:  0.6244\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.373078 Accuracy:  0.6204\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:  0.319092 Accuracy:  0.633\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:  0.268459 Accuracy:  0.6314\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:  0.342021 Accuracy:  0.6248\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:  0.351993 Accuracy:  0.6204\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.362297 Accuracy:  0.6248\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:  0.307625 Accuracy:  0.6308\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:  0.264004 Accuracy:  0.6278\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:  0.336876 Accuracy:  0.634\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:  0.33818 Accuracy:  0.6324\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.33709 Accuracy:  0.632\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:  0.33782 Accuracy:  0.627\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:  0.280051 Accuracy:  0.6206\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:  0.346771 Accuracy:  0.621\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:  0.371988 Accuracy:  0.6268\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.365829 Accuracy:  0.626\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:  0.301972 Accuracy:  0.6314\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:  0.26372 Accuracy:  0.6246\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:  0.369086 Accuracy:  0.611\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:  0.348407 Accuracy:  0.6326\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.344689 Accuracy:  0.6302\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:  0.289436 Accuracy:  0.6302\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:  0.263702 Accuracy:  0.6284\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:  0.339378 Accuracy:  0.6204\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:  0.357614 Accuracy:  0.615\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.323341 Accuracy:  0.6284\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:  0.295949 Accuracy:  0.6344\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:  0.243435 Accuracy:  0.6298\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:  0.365363 Accuracy:  0.6176\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:  0.359489 Accuracy:  0.6256\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  0.345851 Accuracy:  0.634\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:  0.277497 Accuracy:  0.636\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:  0.247921 Accuracy:  0.6352\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:  0.33454 Accuracy:  0.6246\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:  0.370645 Accuracy:  0.6298\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  0.340904 Accuracy:  0.6338\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:  0.304486 Accuracy:  0.6256\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:  0.252403 Accuracy:  0.6344\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:  0.329269 Accuracy:  0.6268\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:  0.343829 Accuracy:  0.628\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.340004 Accuracy:  0.6286\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:  0.298314 Accuracy:  0.6222\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:  0.257425 Accuracy:  0.6312\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:  0.313569 Accuracy:  0.6358\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:  0.352553 Accuracy:  0.6152\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.3109 Accuracy:  0.6354\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:  0.331867 Accuracy:  0.6276\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:  0.266516 Accuracy:  0.6356\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:  0.331536 Accuracy:  0.623\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:  0.344932 Accuracy:  0.6284\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.337677 Accuracy:  0.6306\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:  0.308334 Accuracy:  0.6264\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:  0.242006 Accuracy:  0.6342\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:  0.345866 Accuracy:  0.6204\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:  0.348145 Accuracy:  0.6234\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.340717 Accuracy:  0.6254\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:  0.284359 Accuracy:  0.6216\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:  0.260772 Accuracy:  0.6262\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:  0.323462 Accuracy:  0.6328\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:  0.340203 Accuracy:  0.6196\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.336971 Accuracy:  0.6308\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:  0.285772 Accuracy:  0.6366\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:  0.255426 Accuracy:  0.6284\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:  0.330187 Accuracy:  0.6286\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:  0.324863 Accuracy:  0.6282\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.334511 Accuracy:  0.635\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:  0.286704 Accuracy:  0.6394\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:  0.237363 Accuracy:  0.626\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:  0.328019 Accuracy:  0.6272\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:  0.352347 Accuracy:  0.6238\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.357873 Accuracy:  0.6216\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:  0.283739 Accuracy:  0.6336\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:  0.238107 Accuracy:  0.6336\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:  0.358212 Accuracy:  0.6168\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss:  0.340153 Accuracy:  0.6146\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.321017 Accuracy:  0.6244\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss:  0.269523 Accuracy:  0.6348\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss:  0.241965 Accuracy:  0.63\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss:  0.335127 Accuracy:  0.6286\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss:  0.327615 Accuracy:  0.6252\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.309357 Accuracy:  0.6292\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss:  0.273804 Accuracy:  0.6304\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss:  0.238222 Accuracy:  0.6346\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss:  0.316817 Accuracy:  0.6264\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss:  0.310943 Accuracy:  0.622\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.312133 Accuracy:  0.626\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss:  0.263195 Accuracy:  0.6266\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss:  0.2405 Accuracy:  0.6284\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss:  0.335166 Accuracy:  0.6344\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss:  0.325979 Accuracy:  0.6156\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.327319 Accuracy:  0.6328\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss:  0.277493 Accuracy:  0.6264\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss:  0.216976 Accuracy:  0.6298\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss:  0.313747 Accuracy:  0.6298\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss:  0.345042 Accuracy:  0.6148\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.314947 Accuracy:  0.6378\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss:  0.262795 Accuracy:  0.6374\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss:  0.244727 Accuracy:  0.6306\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss:  0.333976 Accuracy:  0.6182\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss:  0.313106 Accuracy:  0.6316\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.323691 Accuracy:  0.6252\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss:  0.27709 Accuracy:  0.6346\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss:  0.229546 Accuracy:  0.6212\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss:  0.302965 Accuracy:  0.6258\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss:  0.307916 Accuracy:  0.6306\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.318856 Accuracy:  0.6274\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss:  0.270612 Accuracy:  0.6326\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss:  0.220062 Accuracy:  0.6388\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss:  0.302929 Accuracy:  0.6286\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss:  0.324202 Accuracy:  0.6178\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.318987 Accuracy:  0.626\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss:  0.264218 Accuracy:  0.6312\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss:  0.242422 Accuracy:  0.6252\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss:  0.314693 Accuracy:  0.6292\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss:  0.3207 Accuracy:  0.6072\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.319095 Accuracy:  0.6276\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss:  0.246498 Accuracy:  0.6314\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss:  0.22345 Accuracy:  0.6286\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss:  0.30376 Accuracy:  0.6322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210, CIFAR-10 Batch 5:  Loss:  0.311729 Accuracy:  0.625\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.309636 Accuracy:  0.6334\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss:  0.28415 Accuracy:  0.6306\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss:  0.215004 Accuracy:  0.634\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss:  0.277807 Accuracy:  0.633\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss:  0.310492 Accuracy:  0.6276\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.338867 Accuracy:  0.636\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss:  0.260069 Accuracy:  0.636\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss:  0.223327 Accuracy:  0.642\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss:  0.298356 Accuracy:  0.6274\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss:  0.29111 Accuracy:  0.6196\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.295564 Accuracy:  0.6316\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss:  0.242964 Accuracy:  0.633\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss:  0.236582 Accuracy:  0.6236\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss:  0.282883 Accuracy:  0.631\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss:  0.284997 Accuracy:  0.631\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.313846 Accuracy:  0.628\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss:  0.276625 Accuracy:  0.6232\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss:  0.240003 Accuracy:  0.627\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss:  0.33796 Accuracy:  0.62\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss:  0.337009 Accuracy:  0.6124\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.280834 Accuracy:  0.6294\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss:  0.256464 Accuracy:  0.63\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss:  0.237829 Accuracy:  0.6316\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss:  0.302131 Accuracy:  0.6242\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss:  0.305453 Accuracy:  0.6254\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.282331 Accuracy:  0.6208\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss:  0.272073 Accuracy:  0.6292\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss:  0.219186 Accuracy:  0.6302\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss:  0.324794 Accuracy:  0.6128\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss:  0.317852 Accuracy:  0.617\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.29562 Accuracy:  0.6274\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss:  0.242832 Accuracy:  0.636\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss:  0.229082 Accuracy:  0.6216\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss:  0.293805 Accuracy:  0.6242\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss:  0.329871 Accuracy:  0.6178\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.307255 Accuracy:  0.6252\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss:  0.280305 Accuracy:  0.6282\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss:  0.244281 Accuracy:  0.6218\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss:  0.293663 Accuracy:  0.627\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss:  0.310255 Accuracy:  0.6256\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.282387 Accuracy:  0.635\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss:  0.259335 Accuracy:  0.6216\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss:  0.212746 Accuracy:  0.6334\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss:  0.301859 Accuracy:  0.6212\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss:  0.300738 Accuracy:  0.6236\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.315802 Accuracy:  0.6234\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss:  0.25276 Accuracy:  0.6298\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss:  0.231239 Accuracy:  0.6286\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss:  0.3081 Accuracy:  0.6212\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss:  0.303452 Accuracy:  0.6232\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.283776 Accuracy:  0.634\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss:  0.256576 Accuracy:  0.6284\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss:  0.213087 Accuracy:  0.6336\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss:  0.284591 Accuracy:  0.6302\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss:  0.280413 Accuracy:  0.6286\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.283684 Accuracy:  0.6184\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss:  0.234112 Accuracy:  0.6346\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss:  0.206693 Accuracy:  0.62\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss:  0.301885 Accuracy:  0.6238\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss:  0.295881 Accuracy:  0.625\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.313295 Accuracy:  0.6328\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss:  0.234031 Accuracy:  0.6246\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss:  0.228801 Accuracy:  0.6326\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss:  0.286704 Accuracy:  0.6284\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss:  0.289628 Accuracy:  0.6234\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.321732 Accuracy:  0.6278\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss:  0.248085 Accuracy:  0.6238\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss:  0.211624 Accuracy:  0.631\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss:  0.274872 Accuracy:  0.6278\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss:  0.290565 Accuracy:  0.6258\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.295468 Accuracy:  0.6302\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss:  0.23797 Accuracy:  0.6318\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss:  0.210481 Accuracy:  0.628\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss:  0.290481 Accuracy:  0.6312\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss:  0.264512 Accuracy:  0.6256\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.306851 Accuracy:  0.6294\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss:  0.22872 Accuracy:  0.631\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss:  0.206486 Accuracy:  0.6248\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss:  0.275548 Accuracy:  0.6268\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss:  0.285239 Accuracy:  0.6164\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.286612 Accuracy:  0.626\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss:  0.239154 Accuracy:  0.6264\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss:  0.182106 Accuracy:  0.6286\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss:  0.263543 Accuracy:  0.6284\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss:  0.264145 Accuracy:  0.613\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.265486 Accuracy:  0.6166\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss:  0.251244 Accuracy:  0.6316\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss:  0.221223 Accuracy:  0.63\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss:  0.279744 Accuracy:  0.6228\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss:  0.259006 Accuracy:  0.624\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.275288 Accuracy:  0.6328\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss:  0.241113 Accuracy:  0.6208\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss:  0.227814 Accuracy:  0.6256\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss:  0.297962 Accuracy:  0.6312\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss:  0.271731 Accuracy:  0.6206\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.289768 Accuracy:  0.6296\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss:  0.228242 Accuracy:  0.6276\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss:  0.211918 Accuracy:  0.6268\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss:  0.25856 Accuracy:  0.6292\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss:  0.282171 Accuracy:  0.6306\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.280508 Accuracy:  0.6236\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss:  0.230027 Accuracy:  0.6312\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss:  0.220325 Accuracy:  0.6232\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss:  0.2761 Accuracy:  0.6224\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss:  0.278814 Accuracy:  0.6192\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.268658 Accuracy:  0.625\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss:  0.247451 Accuracy:  0.6302\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss:  0.192611 Accuracy:  0.624\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss:  0.26786 Accuracy:  0.6194\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss:  0.288918 Accuracy:  0.6158\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.272936 Accuracy:  0.6148\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss:  0.224783 Accuracy:  0.6378\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss:  0.207131 Accuracy:  0.6296\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss:  0.247611 Accuracy:  0.6314\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss:  0.309844 Accuracy:  0.6144\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.258652 Accuracy:  0.6318\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss:  0.251485 Accuracy:  0.6184\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss:  0.206117 Accuracy:  0.6166\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss:  0.2762 Accuracy:  0.6254\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss:  0.281924 Accuracy:  0.623\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.25285 Accuracy:  0.63\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss:  0.219963 Accuracy:  0.6274\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss:  0.234974 Accuracy:  0.6246\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss:  0.277067 Accuracy:  0.623\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss:  0.289323 Accuracy:  0.6214\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.253984 Accuracy:  0.6148\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss:  0.219494 Accuracy:  0.631\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss:  0.203191 Accuracy:  0.6328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236, CIFAR-10 Batch 4:  Loss:  0.303108 Accuracy:  0.6172\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss:  0.255741 Accuracy:  0.6256\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.277551 Accuracy:  0.628\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss:  0.220195 Accuracy:  0.6268\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss:  0.196462 Accuracy:  0.6314\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss:  0.243891 Accuracy:  0.6328\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss:  0.266757 Accuracy:  0.6184\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.266835 Accuracy:  0.6252\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss:  0.220616 Accuracy:  0.625\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss:  0.201887 Accuracy:  0.6272\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss:  0.264887 Accuracy:  0.6234\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss:  0.254116 Accuracy:  0.6294\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.264612 Accuracy:  0.624\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss:  0.22306 Accuracy:  0.6344\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss:  0.19371 Accuracy:  0.6294\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss:  0.272823 Accuracy:  0.6218\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss:  0.308731 Accuracy:  0.6144\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.259794 Accuracy:  0.628\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss:  0.236132 Accuracy:  0.6276\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss:  0.210322 Accuracy:  0.628\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss:  0.282642 Accuracy:  0.6118\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss:  0.264961 Accuracy:  0.627\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.27495 Accuracy:  0.6266\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss:  0.227132 Accuracy:  0.6266\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss:  0.204847 Accuracy:  0.6266\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss:  0.275519 Accuracy:  0.6224\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss:  0.255849 Accuracy:  0.622\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.24956 Accuracy:  0.6246\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss:  0.224487 Accuracy:  0.6234\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss:  0.193961 Accuracy:  0.6284\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss:  0.262118 Accuracy:  0.6222\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss:  0.243015 Accuracy:  0.6192\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.27301 Accuracy:  0.6216\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss:  0.199227 Accuracy:  0.63\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss:  0.197437 Accuracy:  0.628\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss:  0.243909 Accuracy:  0.629\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss:  0.260515 Accuracy:  0.6174\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.239308 Accuracy:  0.6206\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss:  0.207427 Accuracy:  0.6288\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss:  0.181001 Accuracy:  0.6282\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss:  0.254323 Accuracy:  0.6264\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss:  0.287844 Accuracy:  0.6192\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.251407 Accuracy:  0.62\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss:  0.233149 Accuracy:  0.632\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss:  0.19484 Accuracy:  0.63\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss:  0.267511 Accuracy:  0.6264\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss:  0.249692 Accuracy:  0.6158\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.249628 Accuracy:  0.6328\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss:  0.205095 Accuracy:  0.627\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss:  0.1949 Accuracy:  0.625\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss:  0.261654 Accuracy:  0.6276\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss:  0.252859 Accuracy:  0.6118\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.255484 Accuracy:  0.629\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss:  0.218886 Accuracy:  0.6292\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss:  0.190843 Accuracy:  0.63\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss:  0.277896 Accuracy:  0.6202\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss:  0.24217 Accuracy:  0.6282\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.245296 Accuracy:  0.6174\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss:  0.206379 Accuracy:  0.6328\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss:  0.20656 Accuracy:  0.6236\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss:  0.249783 Accuracy:  0.6204\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss:  0.248449 Accuracy:  0.6178\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.236462 Accuracy:  0.6248\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss:  0.211288 Accuracy:  0.6274\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss:  0.194819 Accuracy:  0.6194\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss:  0.259092 Accuracy:  0.6152\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss:  0.261818 Accuracy:  0.6142\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.246067 Accuracy:  0.6244\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss:  0.235275 Accuracy:  0.6234\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss:  0.186318 Accuracy:  0.628\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss:  0.244897 Accuracy:  0.6198\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss:  0.24432 Accuracy:  0.6226\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.260766 Accuracy:  0.6252\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss:  0.20339 Accuracy:  0.6296\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss:  0.183528 Accuracy:  0.6314\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss:  0.255361 Accuracy:  0.62\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss:  0.242371 Accuracy:  0.6264\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.261491 Accuracy:  0.6204\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss:  0.182435 Accuracy:  0.6334\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss:  0.186953 Accuracy:  0.6236\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss:  0.23601 Accuracy:  0.616\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss:  0.250031 Accuracy:  0.6248\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.245359 Accuracy:  0.622\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss:  0.212143 Accuracy:  0.6276\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss:  0.166657 Accuracy:  0.6348\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss:  0.248287 Accuracy:  0.6268\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss:  0.254373 Accuracy:  0.6198\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.239938 Accuracy:  0.6334\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss:  0.214524 Accuracy:  0.6172\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss:  0.184156 Accuracy:  0.6258\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss:  0.222349 Accuracy:  0.6188\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss:  0.237991 Accuracy:  0.6246\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.22928 Accuracy:  0.6296\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss:  0.208871 Accuracy:  0.6306\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss:  0.175767 Accuracy:  0.6292\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss:  0.245939 Accuracy:  0.6198\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss:  0.223159 Accuracy:  0.6284\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.240849 Accuracy:  0.6254\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss:  0.216145 Accuracy:  0.6296\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss:  0.177915 Accuracy:  0.6272\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss:  0.2386 Accuracy:  0.625\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss:  0.235262 Accuracy:  0.6252\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:  0.217461 Accuracy:  0.6224\n",
      "Epoch 257, CIFAR-10 Batch 2:  Loss:  0.227086 Accuracy:  0.6228\n",
      "Epoch 257, CIFAR-10 Batch 3:  Loss:  0.156327 Accuracy:  0.6246\n",
      "Epoch 257, CIFAR-10 Batch 4:  Loss:  0.244836 Accuracy:  0.6188\n",
      "Epoch 257, CIFAR-10 Batch 5:  Loss:  0.233537 Accuracy:  0.633\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:  0.231582 Accuracy:  0.6288\n",
      "Epoch 258, CIFAR-10 Batch 2:  Loss:  0.234428 Accuracy:  0.6274\n",
      "Epoch 258, CIFAR-10 Batch 3:  Loss:  0.168961 Accuracy:  0.6256\n",
      "Epoch 258, CIFAR-10 Batch 4:  Loss:  0.228426 Accuracy:  0.6282\n",
      "Epoch 258, CIFAR-10 Batch 5:  Loss:  0.242394 Accuracy:  0.6242\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:  0.206183 Accuracy:  0.6228\n",
      "Epoch 259, CIFAR-10 Batch 2:  Loss:  0.187076 Accuracy:  0.6282\n",
      "Epoch 259, CIFAR-10 Batch 3:  Loss:  0.173237 Accuracy:  0.6306\n",
      "Epoch 259, CIFAR-10 Batch 4:  Loss:  0.23565 Accuracy:  0.6232\n",
      "Epoch 259, CIFAR-10 Batch 5:  Loss:  0.258316 Accuracy:  0.6374\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:  0.233973 Accuracy:  0.6196\n",
      "Epoch 260, CIFAR-10 Batch 2:  Loss:  0.201969 Accuracy:  0.6262\n",
      "Epoch 260, CIFAR-10 Batch 3:  Loss:  0.18976 Accuracy:  0.622\n",
      "Epoch 260, CIFAR-10 Batch 4:  Loss:  0.249508 Accuracy:  0.6238\n",
      "Epoch 260, CIFAR-10 Batch 5:  Loss:  0.244232 Accuracy:  0.622\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:  0.22903 Accuracy:  0.619\n",
      "Epoch 261, CIFAR-10 Batch 2:  Loss:  0.23568 Accuracy:  0.6182\n",
      "Epoch 261, CIFAR-10 Batch 3:  Loss:  0.170542 Accuracy:  0.6326\n",
      "Epoch 261, CIFAR-10 Batch 4:  Loss:  0.212073 Accuracy:  0.6316\n",
      "Epoch 261, CIFAR-10 Batch 5:  Loss:  0.240561 Accuracy:  0.6162\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:  0.211118 Accuracy:  0.6204\n",
      "Epoch 262, CIFAR-10 Batch 2:  Loss:  0.189621 Accuracy:  0.629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262, CIFAR-10 Batch 3:  Loss:  0.178537 Accuracy:  0.6214\n",
      "Epoch 262, CIFAR-10 Batch 4:  Loss:  0.248843 Accuracy:  0.6134\n",
      "Epoch 262, CIFAR-10 Batch 5:  Loss:  0.237048 Accuracy:  0.612\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:  0.236876 Accuracy:  0.6112\n",
      "Epoch 263, CIFAR-10 Batch 2:  Loss:  0.218856 Accuracy:  0.634\n",
      "Epoch 263, CIFAR-10 Batch 3:  Loss:  0.194617 Accuracy:  0.614\n",
      "Epoch 263, CIFAR-10 Batch 4:  Loss:  0.219733 Accuracy:  0.6238\n",
      "Epoch 263, CIFAR-10 Batch 5:  Loss:  0.234553 Accuracy:  0.6208\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:  0.200697 Accuracy:  0.6242\n",
      "Epoch 264, CIFAR-10 Batch 2:  Loss:  0.196888 Accuracy:  0.6246\n",
      "Epoch 264, CIFAR-10 Batch 3:  Loss:  0.160383 Accuracy:  0.6226\n",
      "Epoch 264, CIFAR-10 Batch 4:  Loss:  0.228664 Accuracy:  0.6254\n",
      "Epoch 264, CIFAR-10 Batch 5:  Loss:  0.220648 Accuracy:  0.6176\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:  0.223699 Accuracy:  0.6194\n",
      "Epoch 265, CIFAR-10 Batch 2:  Loss:  0.190496 Accuracy:  0.6276\n",
      "Epoch 265, CIFAR-10 Batch 3:  Loss:  0.171264 Accuracy:  0.6214\n",
      "Epoch 265, CIFAR-10 Batch 4:  Loss:  0.222166 Accuracy:  0.6272\n",
      "Epoch 265, CIFAR-10 Batch 5:  Loss:  0.227313 Accuracy:  0.623\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:  0.225344 Accuracy:  0.6242\n",
      "Epoch 266, CIFAR-10 Batch 2:  Loss:  0.202677 Accuracy:  0.612\n",
      "Epoch 266, CIFAR-10 Batch 3:  Loss:  0.170106 Accuracy:  0.6248\n",
      "Epoch 266, CIFAR-10 Batch 4:  Loss:  0.233816 Accuracy:  0.6216\n",
      "Epoch 266, CIFAR-10 Batch 5:  Loss:  0.237432 Accuracy:  0.6248\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:  0.209151 Accuracy:  0.618\n",
      "Epoch 267, CIFAR-10 Batch 2:  Loss:  0.195473 Accuracy:  0.6284\n",
      "Epoch 267, CIFAR-10 Batch 3:  Loss:  0.169606 Accuracy:  0.6164\n",
      "Epoch 267, CIFAR-10 Batch 4:  Loss:  0.203977 Accuracy:  0.6302\n",
      "Epoch 267, CIFAR-10 Batch 5:  Loss:  0.259695 Accuracy:  0.6272\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:  0.228302 Accuracy:  0.6174\n",
      "Epoch 268, CIFAR-10 Batch 2:  Loss:  0.190134 Accuracy:  0.6212\n",
      "Epoch 268, CIFAR-10 Batch 3:  Loss:  0.16455 Accuracy:  0.6232\n",
      "Epoch 268, CIFAR-10 Batch 4:  Loss:  0.235295 Accuracy:  0.6218\n",
      "Epoch 268, CIFAR-10 Batch 5:  Loss:  0.250518 Accuracy:  0.6146\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:  0.249757 Accuracy:  0.619\n",
      "Epoch 269, CIFAR-10 Batch 2:  Loss:  0.188301 Accuracy:  0.6248\n",
      "Epoch 269, CIFAR-10 Batch 3:  Loss:  0.186759 Accuracy:  0.618\n",
      "Epoch 269, CIFAR-10 Batch 4:  Loss:  0.217411 Accuracy:  0.6124\n",
      "Epoch 269, CIFAR-10 Batch 5:  Loss:  0.230153 Accuracy:  0.6306\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:  0.248363 Accuracy:  0.624\n",
      "Epoch 270, CIFAR-10 Batch 2:  Loss:  0.178771 Accuracy:  0.628\n",
      "Epoch 270, CIFAR-10 Batch 3:  Loss:  0.159639 Accuracy:  0.627\n",
      "Epoch 270, CIFAR-10 Batch 4:  Loss:  0.219968 Accuracy:  0.622\n",
      "Epoch 270, CIFAR-10 Batch 5:  Loss:  0.216983 Accuracy:  0.6072\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:  0.250733 Accuracy:  0.6108\n",
      "Epoch 271, CIFAR-10 Batch 2:  Loss:  0.203221 Accuracy:  0.622\n",
      "Epoch 271, CIFAR-10 Batch 3:  Loss:  0.178065 Accuracy:  0.6242\n",
      "Epoch 271, CIFAR-10 Batch 4:  Loss:  0.239077 Accuracy:  0.624\n",
      "Epoch 271, CIFAR-10 Batch 5:  Loss:  0.242094 Accuracy:  0.6138\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:  0.222021 Accuracy:  0.6184\n",
      "Epoch 272, CIFAR-10 Batch 2:  Loss:  0.198553 Accuracy:  0.6204\n",
      "Epoch 272, CIFAR-10 Batch 3:  Loss:  0.172265 Accuracy:  0.6238\n",
      "Epoch 272, CIFAR-10 Batch 4:  Loss:  0.235466 Accuracy:  0.6188\n",
      "Epoch 272, CIFAR-10 Batch 5:  Loss:  0.218375 Accuracy:  0.6224\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:  0.207996 Accuracy:  0.6188\n",
      "Epoch 273, CIFAR-10 Batch 2:  Loss:  0.179063 Accuracy:  0.6208\n",
      "Epoch 273, CIFAR-10 Batch 3:  Loss:  0.184874 Accuracy:  0.6156\n",
      "Epoch 273, CIFAR-10 Batch 4:  Loss:  0.227955 Accuracy:  0.6186\n",
      "Epoch 273, CIFAR-10 Batch 5:  Loss:  0.225605 Accuracy:  0.627\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:  0.215589 Accuracy:  0.6308\n",
      "Epoch 274, CIFAR-10 Batch 2:  Loss:  0.214602 Accuracy:  0.6134\n",
      "Epoch 274, CIFAR-10 Batch 3:  Loss:  0.164399 Accuracy:  0.6216\n",
      "Epoch 274, CIFAR-10 Batch 4:  Loss:  0.199406 Accuracy:  0.6246\n",
      "Epoch 274, CIFAR-10 Batch 5:  Loss:  0.225101 Accuracy:  0.6314\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:  0.230564 Accuracy:  0.6188\n",
      "Epoch 275, CIFAR-10 Batch 2:  Loss:  0.189718 Accuracy:  0.627\n",
      "Epoch 275, CIFAR-10 Batch 3:  Loss:  0.150716 Accuracy:  0.6226\n",
      "Epoch 275, CIFAR-10 Batch 4:  Loss:  0.230843 Accuracy:  0.6124\n",
      "Epoch 275, CIFAR-10 Batch 5:  Loss:  0.230436 Accuracy:  0.617\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:  0.222896 Accuracy:  0.6162\n",
      "Epoch 276, CIFAR-10 Batch 2:  Loss:  0.196229 Accuracy:  0.616\n",
      "Epoch 276, CIFAR-10 Batch 3:  Loss:  0.151606 Accuracy:  0.6176\n",
      "Epoch 276, CIFAR-10 Batch 4:  Loss:  0.217707 Accuracy:  0.6108\n",
      "Epoch 276, CIFAR-10 Batch 5:  Loss:  0.222895 Accuracy:  0.628\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:  0.210281 Accuracy:  0.618\n",
      "Epoch 277, CIFAR-10 Batch 2:  Loss:  0.169326 Accuracy:  0.6246\n",
      "Epoch 277, CIFAR-10 Batch 3:  Loss:  0.149762 Accuracy:  0.6278\n",
      "Epoch 277, CIFAR-10 Batch 4:  Loss:  0.23332 Accuracy:  0.621\n",
      "Epoch 277, CIFAR-10 Batch 5:  Loss:  0.217037 Accuracy:  0.613\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:  0.210729 Accuracy:  0.6246\n",
      "Epoch 278, CIFAR-10 Batch 2:  Loss:  0.184376 Accuracy:  0.6246\n",
      "Epoch 278, CIFAR-10 Batch 3:  Loss:  0.169931 Accuracy:  0.6266\n",
      "Epoch 278, CIFAR-10 Batch 4:  Loss:  0.20308 Accuracy:  0.6224\n",
      "Epoch 278, CIFAR-10 Batch 5:  Loss:  0.218291 Accuracy:  0.6262\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:  0.209858 Accuracy:  0.6158\n",
      "Epoch 279, CIFAR-10 Batch 2:  Loss:  0.209688 Accuracy:  0.6122\n",
      "Epoch 279, CIFAR-10 Batch 3:  Loss:  0.156147 Accuracy:  0.626\n",
      "Epoch 279, CIFAR-10 Batch 4:  Loss:  0.237475 Accuracy:  0.6062\n",
      "Epoch 279, CIFAR-10 Batch 5:  Loss:  0.218423 Accuracy:  0.6214\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:  0.204849 Accuracy:  0.624\n",
      "Epoch 280, CIFAR-10 Batch 2:  Loss:  0.174949 Accuracy:  0.624\n",
      "Epoch 280, CIFAR-10 Batch 3:  Loss:  0.162105 Accuracy:  0.624\n",
      "Epoch 280, CIFAR-10 Batch 4:  Loss:  0.227678 Accuracy:  0.6224\n",
      "Epoch 280, CIFAR-10 Batch 5:  Loss:  0.201454 Accuracy:  0.619\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:  0.215184 Accuracy:  0.6168\n",
      "Epoch 281, CIFAR-10 Batch 2:  Loss:  0.1909 Accuracy:  0.61\n",
      "Epoch 281, CIFAR-10 Batch 3:  Loss:  0.171308 Accuracy:  0.6118\n",
      "Epoch 281, CIFAR-10 Batch 4:  Loss:  0.234368 Accuracy:  0.622\n",
      "Epoch 281, CIFAR-10 Batch 5:  Loss:  0.248934 Accuracy:  0.6168\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:  0.210322 Accuracy:  0.6166\n",
      "Epoch 282, CIFAR-10 Batch 2:  Loss:  0.167326 Accuracy:  0.6256\n",
      "Epoch 282, CIFAR-10 Batch 3:  Loss:  0.170279 Accuracy:  0.617\n",
      "Epoch 282, CIFAR-10 Batch 4:  Loss:  0.205337 Accuracy:  0.6262\n",
      "Epoch 282, CIFAR-10 Batch 5:  Loss:  0.222078 Accuracy:  0.625\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:  0.213057 Accuracy:  0.622\n",
      "Epoch 283, CIFAR-10 Batch 2:  Loss:  0.188472 Accuracy:  0.6058\n",
      "Epoch 283, CIFAR-10 Batch 3:  Loss:  0.147309 Accuracy:  0.6322\n",
      "Epoch 283, CIFAR-10 Batch 4:  Loss:  0.218416 Accuracy:  0.6164\n",
      "Epoch 283, CIFAR-10 Batch 5:  Loss:  0.210942 Accuracy:  0.6246\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:  0.215639 Accuracy:  0.6288\n",
      "Epoch 284, CIFAR-10 Batch 2:  Loss:  0.172251 Accuracy:  0.6216\n",
      "Epoch 284, CIFAR-10 Batch 3:  Loss:  0.145664 Accuracy:  0.6224\n",
      "Epoch 284, CIFAR-10 Batch 4:  Loss:  0.233514 Accuracy:  0.6096\n",
      "Epoch 284, CIFAR-10 Batch 5:  Loss:  0.22823 Accuracy:  0.6152\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:  0.209612 Accuracy:  0.622\n",
      "Epoch 285, CIFAR-10 Batch 2:  Loss:  0.181411 Accuracy:  0.6248\n",
      "Epoch 285, CIFAR-10 Batch 3:  Loss:  0.152 Accuracy:  0.624\n",
      "Epoch 285, CIFAR-10 Batch 4:  Loss:  0.206323 Accuracy:  0.619\n",
      "Epoch 285, CIFAR-10 Batch 5:  Loss:  0.225057 Accuracy:  0.6298\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:  0.192691 Accuracy:  0.6286\n",
      "Epoch 286, CIFAR-10 Batch 2:  Loss:  0.190068 Accuracy:  0.6232\n",
      "Epoch 286, CIFAR-10 Batch 3:  Loss:  0.156738 Accuracy:  0.6156\n",
      "Epoch 286, CIFAR-10 Batch 4:  Loss:  0.204179 Accuracy:  0.6188\n",
      "Epoch 286, CIFAR-10 Batch 5:  Loss:  0.233284 Accuracy:  0.615\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:  0.19184 Accuracy:  0.6168\n",
      "Epoch 287, CIFAR-10 Batch 2:  Loss:  0.165545 Accuracy:  0.618\n",
      "Epoch 287, CIFAR-10 Batch 3:  Loss:  0.145252 Accuracy:  0.6206\n",
      "Epoch 287, CIFAR-10 Batch 4:  Loss:  0.212105 Accuracy:  0.6142\n",
      "Epoch 287, CIFAR-10 Batch 5:  Loss:  0.19502 Accuracy:  0.6214\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:  0.192344 Accuracy:  0.6238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288, CIFAR-10 Batch 2:  Loss:  0.157281 Accuracy:  0.6246\n",
      "Epoch 288, CIFAR-10 Batch 3:  Loss:  0.152309 Accuracy:  0.6198\n",
      "Epoch 288, CIFAR-10 Batch 4:  Loss:  0.216957 Accuracy:  0.623\n",
      "Epoch 288, CIFAR-10 Batch 5:  Loss:  0.207309 Accuracy:  0.6134\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:  0.225631 Accuracy:  0.5956\n",
      "Epoch 289, CIFAR-10 Batch 2:  Loss:  0.172211 Accuracy:  0.6202\n",
      "Epoch 289, CIFAR-10 Batch 3:  Loss:  0.151386 Accuracy:  0.621\n",
      "Epoch 289, CIFAR-10 Batch 4:  Loss:  0.204979 Accuracy:  0.6196\n",
      "Epoch 289, CIFAR-10 Batch 5:  Loss:  0.204209 Accuracy:  0.6116\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:  0.192997 Accuracy:  0.616\n",
      "Epoch 290, CIFAR-10 Batch 2:  Loss:  0.18624 Accuracy:  0.6166\n",
      "Epoch 290, CIFAR-10 Batch 3:  Loss:  0.156298 Accuracy:  0.6204\n",
      "Epoch 290, CIFAR-10 Batch 4:  Loss:  0.189834 Accuracy:  0.6124\n",
      "Epoch 290, CIFAR-10 Batch 5:  Loss:  0.211058 Accuracy:  0.6208\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:  0.18437 Accuracy:  0.6162\n",
      "Epoch 291, CIFAR-10 Batch 2:  Loss:  0.175711 Accuracy:  0.6248\n",
      "Epoch 291, CIFAR-10 Batch 3:  Loss:  0.15398 Accuracy:  0.6146\n",
      "Epoch 291, CIFAR-10 Batch 4:  Loss:  0.184444 Accuracy:  0.6206\n",
      "Epoch 291, CIFAR-10 Batch 5:  Loss:  0.209768 Accuracy:  0.6156\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:  0.189624 Accuracy:  0.6182\n",
      "Epoch 292, CIFAR-10 Batch 2:  Loss:  0.161815 Accuracy:  0.618\n",
      "Epoch 292, CIFAR-10 Batch 3:  Loss:  0.146306 Accuracy:  0.622\n",
      "Epoch 292, CIFAR-10 Batch 4:  Loss:  0.214312 Accuracy:  0.6078\n",
      "Epoch 292, CIFAR-10 Batch 5:  Loss:  0.201513 Accuracy:  0.6234\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:  0.185614 Accuracy:  0.6134\n",
      "Epoch 293, CIFAR-10 Batch 2:  Loss:  0.169201 Accuracy:  0.6182\n",
      "Epoch 293, CIFAR-10 Batch 3:  Loss:  0.163555 Accuracy:  0.6192\n",
      "Epoch 293, CIFAR-10 Batch 4:  Loss:  0.190458 Accuracy:  0.6162\n",
      "Epoch 293, CIFAR-10 Batch 5:  Loss:  0.214568 Accuracy:  0.6124\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:  0.180944 Accuracy:  0.619\n",
      "Epoch 294, CIFAR-10 Batch 2:  Loss:  0.169788 Accuracy:  0.6116\n",
      "Epoch 294, CIFAR-10 Batch 3:  Loss:  0.14294 Accuracy:  0.626\n",
      "Epoch 294, CIFAR-10 Batch 4:  Loss:  0.201688 Accuracy:  0.623\n",
      "Epoch 294, CIFAR-10 Batch 5:  Loss:  0.22036 Accuracy:  0.6184\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:  0.197608 Accuracy:  0.6204\n",
      "Epoch 295, CIFAR-10 Batch 2:  Loss:  0.176475 Accuracy:  0.6236\n",
      "Epoch 295, CIFAR-10 Batch 3:  Loss:  0.152782 Accuracy:  0.611\n",
      "Epoch 295, CIFAR-10 Batch 4:  Loss:  0.197221 Accuracy:  0.6156\n",
      "Epoch 295, CIFAR-10 Batch 5:  Loss:  0.204441 Accuracy:  0.6126\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:  0.213519 Accuracy:  0.6174\n",
      "Epoch 296, CIFAR-10 Batch 2:  Loss:  0.152618 Accuracy:  0.6246\n",
      "Epoch 296, CIFAR-10 Batch 3:  Loss:  0.151994 Accuracy:  0.6218\n",
      "Epoch 296, CIFAR-10 Batch 4:  Loss:  0.224345 Accuracy:  0.5918\n",
      "Epoch 296, CIFAR-10 Batch 5:  Loss:  0.209911 Accuracy:  0.6192\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:  0.190487 Accuracy:  0.609\n",
      "Epoch 297, CIFAR-10 Batch 2:  Loss:  0.165207 Accuracy:  0.6276\n",
      "Epoch 297, CIFAR-10 Batch 3:  Loss:  0.155191 Accuracy:  0.6132\n",
      "Epoch 297, CIFAR-10 Batch 4:  Loss:  0.207642 Accuracy:  0.6084\n",
      "Epoch 297, CIFAR-10 Batch 5:  Loss:  0.193644 Accuracy:  0.6158\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:  0.191789 Accuracy:  0.6178\n",
      "Epoch 298, CIFAR-10 Batch 2:  Loss:  0.178733 Accuracy:  0.6156\n",
      "Epoch 298, CIFAR-10 Batch 3:  Loss:  0.133654 Accuracy:  0.6214\n",
      "Epoch 298, CIFAR-10 Batch 4:  Loss:  0.194991 Accuracy:  0.6258\n",
      "Epoch 298, CIFAR-10 Batch 5:  Loss:  0.202655 Accuracy:  0.6256\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:  0.206525 Accuracy:  0.6202\n",
      "Epoch 299, CIFAR-10 Batch 2:  Loss:  0.178939 Accuracy:  0.617\n",
      "Epoch 299, CIFAR-10 Batch 3:  Loss:  0.150866 Accuracy:  0.6218\n",
      "Epoch 299, CIFAR-10 Batch 4:  Loss:  0.190253 Accuracy:  0.6144\n",
      "Epoch 299, CIFAR-10 Batch 5:  Loss:  0.216785 Accuracy:  0.617\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:  0.18895 Accuracy:  0.6206\n",
      "Epoch 300, CIFAR-10 Batch 2:  Loss:  0.16993 Accuracy:  0.62\n",
      "Epoch 300, CIFAR-10 Batch 3:  Loss:  0.137888 Accuracy:  0.6202\n",
      "Epoch 300, CIFAR-10 Batch 4:  Loss:  0.191061 Accuracy:  0.6218\n",
      "Epoch 300, CIFAR-10 Batch 5:  Loss:  0.189401 Accuracy:  0.6254\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6190664556962026\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49be1a0860>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
